[
    [
        "2307.09288",
        "https://arxiv.org/abs/2307.09288",
        "https://arxiv.org/pdf/2307.09288.pdf",
        "18 Jul 2023",
        "Llama 2: Open Foundation and Fine-Tuned Chat Models",
        [
            "Hugo Touvron",
            "Louis Martin",
            "Kevin Stone",
            "Peter Albert",
            "Amjad Almahairi",
            "Yasmine Babaei",
            "Nikolay Bashlykov",
            "Soumya Batra",
            "Prajjwal Bhargava",
            "Shruti Bhosale",
            "Dan Bikel",
            "Lukas Blecher",
            "Cristian Canton Ferrer",
            "Moya Chen",
            "Guillem Cucurull",
            "David Esiobu",
            "Jude Fernandes",
            "Jeremy Fu",
            "Wenyin Fu",
            "Brian Fuller",
            "Cynthia Gao",
            "Vedanuj Goswami",
            "Naman Goyal",
            "Anthony Hartshorn",
            "Saghar Hosseini",
            "Rui Hou",
            "Hakan Inan",
            "Marcin Kardas",
            "Viktor Kerkez",
            "Madian Khabsa",
            "Isabel Kloumann",
            "Artem Korenev",
            "Punit Singh Koura",
            "Marie-Anne Lachaux",
            "Thibaut Lavril",
            "Jenya Lee",
            "Diana Liskovich",
            "Yinghai Lu",
            "Yuning Mao",
            "Xavier Martinet",
            "Todor Mihaylov",
            "Pushkar Mishra",
            "Igor Molybog",
            "Yixin Nie",
            "Andrew Poulton",
            "Jeremy Reizenstein",
            "Rashi Rungta",
            "Kalyan Saladi",
            "Alan Schelten",
            "Ruan Silva",
            "Eric Michael Smith",
            "Ranjan Subramanian",
            "Xiaoqing Ellen Tan",
            "Binh Tang",
            "Ross Taylor",
            "Adina Williams",
            "Jian Xiang Kuan",
            "Puxin Xu",
            "Zheng Yan",
            "Iliyan Zarov",
            "Yuchen Zhang",
            "Angela Fan",
            "Melanie Kambadur",
            "Sharan Narang",
            "Aurelien Rodriguez",
            "Robert Stojnic",
            "Sergey Edunov",
            "Thomas Scialom"
        ],
        "In this work, we develop and release Llama 2, a collection of pretrained and\nfine-tuned large language models (LLMs) ranging in scale from 7 billion to 70\nbillion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for\ndialogue use cases. Our models outperform open-source chat models on most\nbenchmarks we tested, and based on our human evaluations for helpfulness and\nsafety, may be a suitable substitute for closed-source models. We provide a\ndetailed description of our approach to fine-tuning and safety improvements of\nLlama 2-Chat in order to enable the community to build on our work and\ncontribute to the responsible development of LLMs.",
        "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/2B_OeMWtjVHEfBRJOvXbF.png"
    ],
    [
        "2307.09009",
        "https://arxiv.org/abs/2307.09009",
        "https://arxiv.org/pdf/2307.09009.pdf",
        "18 Jul 2023",
        "How is ChatGPT's behavior changing over time?",
        [
            "Lingjiao Chen",
            "Matei Zaharia",
            "James Zou"
        ],
        "GPT-3.5 and GPT-4 are the two most widely used large language model (LLM)\nservices. However, when and how these models are updated over time is opaque.\nHere, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on\nfour diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous\nquestions, 3) generating code and 4) visual reasoning. We find that the\nperformance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time.\nFor example, GPT-4 (March 2023) was very good at identifying prime numbers\n(accuracy 97.6%) but GPT-4 (June 2023) was very poor on these same questions\n(accuracy 2.4%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5\n(March 2023) in this task. GPT-4 was less willing to answer sensitive questions\nin June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes\nin code generation in June than in March. Overall, our findings shows that the\nbehavior of the same LLM service can change substantially in a relatively short\namount of time, highlighting the need for continuous monitoring of LLM quality.",
        "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/fQiXHcIbmpzpwqmt6_MwR.png"
    ],
    [
        "2307.09112",
        "https://arxiv.org/abs/2307.09112",
        "https://arxiv.org/pdf/2307.09112.pdf",
        "18 Jul 2023",
        "NU-MCC: Multiview Compressive Coding with Neighborhood Decoder and Repulsive UDF",
        [
            "Stefan Lionar",
            "Xiangyu Xu",
            "Min Lin",
            "Gim Hee Lee"
        ],
        "Remarkable progress has been made in 3D reconstruction from single-view RGB-D\ninputs. MCC is the current state-of-the-art method in this field, which\nachieves unprecedented success by combining vision Transformers with\nlarge-scale training. However, we identified two key limitations of MCC: 1) The\nTransformer decoder is inefficient in handling large number of query points; 2)\nThe 3D representation struggles to recover high-fidelity details. In this\npaper, we propose a new approach called NU-MCC that addresses these\nlimitations. NU-MCC includes two key innovations: a Neighborhood decoder and a\nRepulsive Unsigned Distance Function (Repulsive UDF). First, our Neighborhood\ndecoder introduces center points as an efficient proxy of input visual\nfeatures, allowing each query point to only attend to a small neighborhood.\nThis design not only results in much faster inference speed but also enables\nthe exploitation of finer-scale visual features for improved recovery of 3D\ntextures. Second, our Repulsive UDF is a novel alternative to the occupancy\nfield used in MCC, significantly improving the quality of 3D object\nreconstruction. Compared to standard UDFs that suffer from holes in results,\nour proposed Repulsive UDF can achieve more complete surface reconstruction.\nExperimental results demonstrate that NU-MCC is able to learn a strong 3D\nrepresentation, significantly advancing the state of the art in single-view 3D\nreconstruction. Particularly, it outperforms MCC by 9.7% in terms of the\nF1-score on the CO3D-v2 dataset with more than 5x faster running speed.",
        "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xChAVyTMLL5xorZeUw7kU.mp4"
    ],
    [
        "2307.09458",
        "https://arxiv.org/abs/2307.09458",
        "https://arxiv.org/pdf/2307.09458.pdf",
        "18 Jul 2023",
        "Does Circuit Analysis Interpretability Scale? Evidence from Multiple Choice Capabilities in Chinchilla",
        [
            "Tom Lieberum",
            "Matthew Rahtz",
            "J\u00e1nos Kram\u00e1r",
            "Geoffrey Irving",
            "Rohin Shah",
            "Vladimir Mikulik"
        ],
        "\\emph{Circuit analysis} is a promising technique for understanding the\ninternal mechanisms of language models. However, existing analyses are done in\nsmall models far from the state of the art. To address this, we present a case\nstudy of circuit analysis in the 70B Chinchilla model, aiming to test the\nscalability of circuit analysis. In particular, we study multiple-choice\nquestion answering, and investigate Chinchilla's capability to identify the\ncorrect answer \\emph{label} given knowledge of the correct answer \\emph{text}.\nWe find that the existing techniques of logit attribution, attention pattern\nvisualization, and activation patching naturally scale to Chinchilla, allowing\nus to identify and categorize a small set of `output nodes' (attention heads\nand MLPs).\nWe further study the `correct letter' category of attention heads aiming to\nunderstand the semantics of their features, with mixed results. For normal\nmultiple-choice question answers, we significantly compress the query, key and\nvalue subspaces of the head without loss of performance when operating on the\nanswer labels for multiple-choice questions, and we show that the query and key\nsubspaces represent an `Nth item in an enumeration' feature to at least some\nextent. However, when we attempt to use this explanation to understand the\nheads' behaviour on a more general distribution including randomized answer\nlabels, we find that it is only a partial explanation, suggesting there is more\nto learn about the operation of `correct letter' heads on multiple choice\nquestion answering.",
        "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/74A8TPWrOBYNsPhhx3sSD.png"
    ],
    [
        "2307.09320",
        "https://arxiv.org/abs/2307.09320",
        "https://arxiv.org/pdf/2307.09320.pdf",
        "18 Jul 2023",
        "Biomaker CA: a Biome Maker project using Cellular Automata",
        [
            "Ettore Randazzo",
            "Alexander Mordvintsev"
        ],
        "We introduce Biomaker CA: a Biome Maker project using Cellular Automata (CA).\nIn Biomaker CA, morphogenesis is a first class citizen and small seeds need to\ngrow into plant-like organisms to survive in a nutrient starved environment and\neventually reproduce with variation so that a biome survives for long\ntimelines. We simulate complex biomes by means of CA rules in 2D grids and\nparallelize all of its computation on GPUs through the Python JAX framework. We\nshow how this project allows for several different kinds of environments and\nlaws of 'physics', alongside different model architectures and mutation\nstrategies. We further analyze some configurations to show how plant agents can\ngrow, survive, reproduce, and evolve, forming stable and unstable biomes. We\nthen demonstrate how one can meta-evolve models to survive in a harsh\nenvironment either through end-to-end meta-evolution or by a more surgical and\nefficient approach, called Petri dish meta-evolution. Finally, we show how to\nperform interactive evolution, where the user decides how to evolve a plant\nmodel interactively and then deploys it in a larger environment. We open source\nBiomaker CA at: this https URL .",
        "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/o90qeRcNN7WbLp-rWVfGI.png"
    ],
    [
        "2307.09233",
        "https://arxiv.org/abs/2307.09233",
        "https://arxiv.org/pdf/2307.09233.pdf",
        "18 Jul 2023",
        "Augmenting CLIP with Improved Visio-Linguistic Reasoning",
        [
            "Samyadeep Basu",
            "Maziar Sanjabi",
            "Daniela Massiceti",
            "Shell Xu Hu",
            "Soheil Feizi"
        ],
        "Image-text contrastive models such as CLIP are useful for a variety of\ndownstream applications including zero-shot classification, image-text\nretrieval and transfer learning. However, these contrastively trained\nvision-language models often fail on compositional visio-linguistic tasks such\nas Winoground with performance equivalent to random chance. In our paper, we\naddress this issue and propose a sample-efficient light-weight method called\nSDS-CLIP to improve the compositional visio-linguistic reasoning capabilities\nof CLIP. The core idea of our method is to use differentiable image\nparameterizations to fine-tune CLIP with a distillation objective from large\ntext-to-image generative models such as Stable-Diffusion which are relatively\ngood at visio-linguistic reasoning tasks. On the challenging Winoground\ncompositional reasoning benchmark, our method improves the absolute\nvisio-linguistic performance of different CLIP models by up to 7%, while on the\nARO dataset, our method improves the visio-linguistic performance by upto 3%.\nAs a byproduct of inducing visio-linguistic reasoning into CLIP, we also find\nthat the zero-shot performance improves marginally on a variety of downstream\ndatasets. Our method reinforces that carefully designed distillation objectives\nfrom generative models can be leveraged to extend existing contrastive\nimage-text models with improved visio-linguistic reasoning capabilities.",
        "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/W3fOyJjMSmQH3oiDGD6wX.png"
    ]
]