[
    {
        "arxivID": "2307.08621",
        "pdf_url": "https://arxiv.org/pdf/2307.08621.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08621",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/XGiFVyacRqnARSZSxrIDU.png",
        "media": null,
        "field": "Computation and Language",
        "title": "Retentive Network: A Successor to Transformer for Large Language Models",
        "authors": [
            "Yutao Sun",
            "Li Dong",
            "Shaohan Huang",
            "Shuming Ma",
            "Yuqing Xia",
            "Jilong Xue",
            "Jianyong Wang",
            "Furu Wei"
        ],
        "desc": "In this work, we propose Retentive Network (RetNet) as a foundation\narchitecture for large language models, simultaneously achieving training\nparallelism, low-cost inference, and good performance. We theoretically derive\nthe connection between recurrence and attention. Then we propose the retention\nmechanism for sequence modeling, which supports three computation paradigms,\ni.e., parallel, recurrent, and chunkwise recurrent. Specifically, the parallel\nrepresentation allows for training parallelism. The recurrent representation\nenables low-cost $O(1)$ inference, which improves decoding throughput, latency,\nand GPU memory without sacrificing performance. The chunkwise recurrent\nrepresentation facilitates efficient long-sequence modeling with linear\ncomplexity, where each chunk is encoded parallelly while recurrently\nsummarizing the chunks. Experimental results on language modeling show that\nRetNet achieves favorable scaling results, parallel training, low-cost\ndeployment, and efficient inference. The intriguing properties make RetNet a\nstrong successor to Transformer for large language models. Code will be\navailable at this https URL.",
        "submission": "2023-07-17T16:40:01.000000Z"
    },
    {
        "arxivID": "2307.08581",
        "pdf_url": "https://arxiv.org/pdf/2307.08581.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08581",
        "cover": null,
        "media": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/jEUds1HYHLBM3inM97Emj.mp4",
        "field": "Computer Vision and Pattern Recognition",
        "title": "BuboGPT: Enabling Visual Grounding in Multi-Modal LLMs",
        "authors": [
            "Yang Zhao",
            "Zhijie Lin",
            "Daquan Zhou",
            "Zilong Huang",
            "Jiashi Feng",
            "Bingyi Kang"
        ],
        "desc": "LLMs have demonstrated remarkable abilities at interacting with humans\nthrough language, especially with the usage of instruction-following data.\nRecent advancements in LLMs, such as MiniGPT-4, LLaVA, and X-LLM, further\nenlarge their abilities by incorporating multi-modal inputs, including image,\nvideo, and speech. Despite their effectiveness at generating precise and\ndetailed language understanding of the given modality signal, these LLMs give\nup the ability to ground specific parts of inputs, thus only constructing a\ncoarse-grained mapping. However, explicit and informative correspondence\nbetween text and other modalities will not only improve the user experience but\nalso help to expand the application scenario of multi-modal LLMs. Therefore, we\npropose BuboGPT, a multi-modal LLM with visual grounding that can perform\ncross-modal interaction between vision, audio and language, providing\nfine-grained understanding of visual objects and other given modalities. As a\nresult, BuboGPT is able to point out the specific location of an object in the\nimage, when it is generating response or description for that object. Our\ncontributions are two-fold: 1) An off-the-shelf visual grounding module based\non SAM that extracts entities in a sentence and find corresponding masks in the\nimage. 2) A two-stage training scheme and instruction dataset to endow joint\ntext-image-audio understanding. Our experiments show that BuboGPT achieves\nimpressive multi-modality understanding and visual grounding abilities during\nthe interaction with human. It performs consistently well when provided by\narbitrary modality combinations (either aligned or unaligned). Our code, model\nand dataset are available at this https URL .",
        "submission": "2023-07-17T15:51:47.000000Z"
    },
    {
        "arxivID": "2307.08674",
        "pdf_url": "https://arxiv.org/pdf/2307.08674.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08674",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/AKTnbVqCICvNWLOb8IgIL.png",
        "media": null,
        "field": "Artificial Intelligence",
        "title": "TableGPT: Towards Unifying Tables, Nature Language and Commands into One GPT",
        "authors": [
            "Liangyu Zha",
            "Junlin Zhou",
            "Liyao Li",
            "Rui Wang",
            "Qingyi Huang",
            "Saisai Yang",
            "Jing Yuan",
            "Changbao Su",
            "Xiang Li",
            "Aofeng Su",
            "Tao Zhang",
            "Chen Zhou",
            "Kaizhe Shou",
            "Miao Wang",
            "Wufang Zhu",
            "Guoshan Lu",
            "Chao Ye",
            "Yali Ye",
            "Wentao Ye",
            "Yiming Zhang",
            "Xinglong Deng",
            "Jie Xu",
            "Haobo Wang",
            "Gang Chen",
            "Junbo Zhao"
        ],
        "desc": "Tables are prevalent in real-world databases, requiring significant time and\neffort for humans to analyze and manipulate. The advancements in large language\nmodels (LLMs) have made it possible to interact with tables using natural\nlanguage input, bringing this capability closer to reality. In this paper, we\npresent TableGPT, a unified fine-tuned framework that enables LLMs to\nunderstand and operate on tables using external functional commands. It\nintroduces the capability to seamlessly interact with tables, enabling a wide\nrange of functionalities such as question answering, data manipulation (e.g.,\ninsert, delete, query, and modify operations), data visualization, analysis\nreport generation, and automated prediction. TableGPT aims to provide\nconvenience and accessibility to users by empowering them to effortlessly\nleverage tabular data. At the core of TableGPT lies the novel concept of global\ntabular representations, which empowers LLMs to gain a comprehensive\nunderstanding of the entire table beyond meta-information. By jointly training\nLLMs on both table and text modalities, TableGPT achieves a deep understanding\nof tabular data and the ability to perform complex operations on tables through\nchain-of-command instructions. Importantly, TableGPT offers the advantage of\nbeing a self-contained system rather than relying on external API interfaces.\nMoreover, it supports efficient data process flow, query rejection (when\nappropriate) and private deployment, enabling faster domain data fine-tuning\nand ensuring data privacy, which enhances the framework's adaptability to\nspecific use cases.",
        "submission": "2023-07-17T17:36:09.000000Z"
    },
    {
        "arxivID": "2307.08701",
        "pdf_url": "https://arxiv.org/pdf/2307.08701.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08701",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/zNtHKGFfMPPwzhcd-9cPK.png",
        "media": null,
        "field": "Computation and Language",
        "title": "AlpaGasus: Training A Better Alpaca with Fewer Data",
        "authors": [
            "Lichang Chen",
            "Shiyang Li",
            "Jun Yan",
            "Hai Wang",
            "Kalpa Gunaratna",
            "Vikas Yadav",
            "Zheng Tang",
            "Vijay Srinivasan",
            "Tianyi Zhou",
            "Heng Huang",
            "Hongxia Jin"
        ],
        "desc": "Large language models~(LLMs) obtain instruction-following capability through\ninstruction-finetuning (IFT) on supervised instruction/response data. However,\nwidely used IFT datasets (e.g., Alpaca's 52k data) surprisingly contain many\nlow-quality instances with incorrect or irrelevant responses, which are\nmisleading and detrimental to IFT. In this paper, we propose a simple and\neffective data selection strategy that automatically identifies and removes\nlow-quality data using a strong LLM (e.g., ChatGPT). To this end, we introduce\nAlpaGasus, which is finetuned on only 9k high-quality data filtered from the\n52k Alpaca data. AlpaGasus significantly outperforms the original Alpaca as\nevaluated by GPT-4 on multiple test sets and its 13B variant matches $>90\\%$\nperformance of its teacher LLM (i.e., Text-Davinci-003) on test tasks. It also\nprovides 5.7x faster training, reducing the training time for a 7B variant from\n80 minutes (for Alpaca) to 14 minutes \\footnote{We apply IFT for the same\nnumber of epochs as Alpaca(7B) but on fewer data, using 4$\\times$NVIDIA A100\n(80GB) GPUs and following the original Alpaca setting and hyperparameters.}.\nOverall, AlpaGasus demonstrates a novel data-centric IFT paradigm that can be\ngenerally applied to instruction-tuning data, leading to faster training and\nbetter instruction-following models. Our project page is available at:\n\\url{this https URL}.",
        "submission": "2023-07-17T17:59:40.000000Z"
    },
    {
        "arxivID": "2307.08702",
        "pdf_url": "https://arxiv.org/pdf/2307.08702.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08702",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/ZMAkBleRclou7CNxvbist.png",
        "media": null,
        "field": "Computer Vision and Pattern Recognition",
        "title": "Diffusion Models Beat GANs on Image Classification",
        "authors": [
            "Soumik Mukhopadhyay",
            "Matthew Gwilliam",
            "Vatsal Agarwal",
            "Namitha Padmanabhan",
            "Archana Swaminathan",
            "Srinidhi Hegde",
            "Tianyi Zhou",
            "Abhinav Shrivastava"
        ],
        "desc": "While many unsupervised learning models focus on one family of tasks, either\ngenerative or discriminative, we explore the possibility of a unified\nrepresentation learner: a model which uses a single pre-training stage to\naddress both families of tasks simultaneously. We identify diffusion models as\na prime candidate. Diffusion models have risen to prominence as a\nstate-of-the-art method for image generation, denoising, inpainting,\nsuper-resolution, manipulation, etc. Such models involve training a U-Net to\niteratively predict and remove noise, and the resulting model can synthesize\nhigh fidelity, diverse, novel images. The U-Net architecture, as a\nconvolution-based architecture, generates a diverse set of feature\nrepresentations in the form of intermediate feature maps. We present our\nfindings that these embeddings are useful beyond the noise prediction task, as\nthey contain discriminative information and can also be leveraged for\nclassification. We explore optimal methods for extracting and using these\nembeddings for classification tasks, demonstrating promising results on the\nImageNet classification task. We find that with careful feature selection and\npooling, diffusion models outperform comparable generative-discriminative\nmethods such as BigBiGAN for classification tasks. We investigate diffusion\nmodels in the transfer learning regime, examining their performance on several\nfine-grained visual classification datasets. We compare these embeddings to\nthose generated by competing architectures and pre-trainings for classification\ntasks.",
        "submission": "2023-07-17T17:59:40.000000Z"
    },
    {
        "arxivID": "2307.07635",
        "pdf_url": "https://arxiv.org/pdf/2307.07635.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.07635",
        "cover": null,
        "media": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/rPWtpV7ZxTJQ4KDj7oktH.mp4",
        "field": "Computer Vision and Pattern Recognition",
        "title": "CoTracker: It is Better to Track Together",
        "authors": [
            "Nikita Karaev",
            "Ignacio Rocco",
            "Benjamin Graham",
            "Natalia Neverova",
            "Andrea Vedaldi",
            "Christian Rupprecht"
        ],
        "desc": "Methods for video motion prediction either estimate jointly the instantaneous\nmotion of all points in a given video frame using optical flow or independently\ntrack the motion of individual points throughout the video. The latter is true\neven for powerful deep-learning methods that can track points through\nocclusions. Tracking points individually ignores the strong correlation that\ncan exist between the points, for instance, because they belong to the same\nphysical object, potentially harming performance. In this paper, we thus\npropose CoTracker, an architecture that jointly tracks multiple points\nthroughout an entire video. This architecture combines several ideas from the\noptical flow and tracking literature in a new, flexible and powerful design. It\nis based on a transformer network that models the correlation of different\npoints in time via specialised attention layers. The transformer iteratively\nupdates an estimate of several trajectories. It can be applied in a\nsliding-window manner to very long videos, for which we engineer an unrolled\ntraining loop. It can track from one to several points jointly and supports\nadding new points to track at any time. The result is a flexible and powerful\ntracking algorithm that outperforms state-of-the-art methods in almost all\nbenchmarks.",
        "submission": "2023-07-14T21:13:04.000000Z"
    },
    {
        "arxivID": "2307.08041",
        "pdf_url": "https://arxiv.org/pdf/2307.08041.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08041",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/NXbzU0OtfSW8c06Mc2hhH.png",
        "media": null,
        "field": "Computer Vision and Pattern Recognition",
        "title": "Planting a SEED of Vision in Large Language Model",
        "authors": [
            "Yuying Ge",
            "Yixiao Ge",
            "Ziyun Zeng",
            "Xintao Wang",
            "Ying Shan"
        ],
        "desc": "We present SEED, an elaborate image tokenizer that empowers Large Language\nModels (LLMs) with the emergent ability to SEE and Draw at the same time.\nResearch on image tokenizers has previously reached an impasse, as frameworks\nemploying quantized visual tokens have lost prominence due to subpar\nperformance and convergence in multimodal comprehension (compared to BLIP-2,\netc.) or generation (compared to Stable Diffusion, etc.). Despite the\nlimitations, we remain confident in its natural capacity to unify visual and\ntextual representations, facilitating scalable multimodal training with LLM's\noriginal recipe. In this study, we identify two crucial principles for the\narchitecture and training of SEED that effectively ease subsequent alignment\nwith LLMs. (1) Image tokens should be independent of 2D physical patch\npositions and instead be produced with a 1D causal dependency, exhibiting\nintrinsic interdependence that aligns with the left-to-right autoregressive\nprediction mechanism in LLMs. (2) Image tokens should capture high-level\nsemantics consistent with the degree of semantic abstraction in words, and be\noptimized for both discriminativeness and reconstruction during the tokenizer\ntraining phase. As a result, the off-the-shelf LLM is able to perform both\nimage-to-text and text-to-image generation by incorporating our SEED through\nefficient LoRA tuning. Comprehensive multimodal pretraining and instruction\ntuning, which may yield improved results, are reserved for future\ninvestigation. This version of SEED was trained in 5.7 days using only 64 V100\nGPUs and 5M publicly available image-text pairs. Our preliminary study\nemphasizes the great potential of discrete visual tokens in versatile\nmultimodal LLMs and the importance of proper image tokenizers in broader\nresearch.",
        "submission": "2023-07-16T13:41:39.000000Z"
    },
    {
        "arxivID": "2307.08506",
        "pdf_url": "https://arxiv.org/pdf/2307.08506.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08506",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/kJjW7MqJsvzPCP0RhBu6y.png",
        "media": null,
        "field": "Computer Vision and Pattern Recognition",
        "title": "Does Visual Pretraining Help End-to-End Reasoning?",
        "authors": [
            "Chen Sun",
            "Calvin Luo",
            "Xingyi Zhou",
            "Anurag Arnab",
            "Cordelia Schmid"
        ],
        "desc": "We aim to investigate whether end-to-end learning of visual reasoning can be\nachieved with general-purpose neural networks, with the help of visual\npretraining. A positive result would refute the common belief that explicit\nvisual abstraction (e.g. object detection) is essential for compositional\ngeneralization on visual reasoning, and confirm the feasibility of a neural\nnetwork \"generalist\" to solve visual recognition and reasoning tasks. We\npropose a simple and general self-supervised framework which \"compresses\" each\nvideo frame into a small set of tokens with a transformer network, and\nreconstructs the remaining frames based on the compressed temporal context. To\nminimize the reconstruction loss, the network must learn a compact\nrepresentation for each image, as well as capture temporal dynamics and object\npermanence from temporal context. We perform evaluation on two visual reasoning\nbenchmarks, CATER and ACRE. We observe that pretraining is essential to achieve\ncompositional generalization for end-to-end visual reasoning. Our proposed\nframework outperforms traditional supervised pretraining, including image\nclassification and explicit object detection, by large margins.",
        "submission": "2023-07-17T14:08:38.000000Z"
    },
    {
        "arxivID": "2307.07663",
        "pdf_url": "https://arxiv.org/pdf/2307.07663.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.07663",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/99Hhj9bJ5WxfHCm9LrxQs.png",
        "media": null,
        "field": "Computer Vision and Pattern Recognition",
        "title": "INVE: Interactive Neural Video Editing",
        "authors": [
            "Jiahui Huang",
            "Leonid Sigal",
            "Kwang Moo Yi",
            "Oliver Wang",
            "Joon-Young Lee"
        ],
        "desc": "We present Interactive Neural Video Editing (INVE), a real-time video editing\nsolution, which can assist the video editing process by consistently\npropagating sparse frame edits to the entire video clip. Our method is inspired\nby the recent work on Layered Neural Atlas (LNA). LNA, however, suffers from\ntwo major drawbacks: (1) the method is too slow for interactive editing, and\n(2) it offers insufficient support for some editing use cases, including direct\nframe editing and rigid texture tracking. To address these challenges we\nleverage and adopt highly efficient network architectures, powered by\nhash-grids encoding, to substantially improve processing speed. In addition, we\nlearn bi-directional functions between image-atlas and introduce vectorized\nediting, which collectively enables a much greater variety of edits in both the\natlas and the frames directly. Compared to LNA, our INVE reduces the learning\nand inference time by a factor of 5, and supports various video editing\noperations that LNA cannot. We showcase the superiority of INVE over LNA in\ninteractive video editing through a comprehensive quantitative and qualitative\nanalysis, highlighting its numerous advantages and improved performance. For\nvideo results, please see this https URL",
        "submission": "2023-07-15T00:02:41.000000Z"
    },
    {
        "arxivID": "2307.08579",
        "pdf_url": "https://arxiv.org/pdf/2307.08579.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.08579",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/JbjVxcH-94T5U-M4nT5Pl.png",
        "media": null,
        "field": "Computer Vision and Pattern Recognition",
        "title": "Scale-Aware Modulation Meet Transformer",
        "authors": [
            "Weifeng Lin",
            "Ziheng Wu",
            "Jiayu Chen",
            "Jun Huang",
            "Lianwen Jin"
        ],
        "desc": "This paper presents a new vision Transformer, Scale-Aware Modulation\nTransformer (SMT), that can handle various downstream tasks efficiently by\ncombining the convolutional network and vision Transformer. The proposed\nScale-Aware Modulation (SAM) in the SMT includes two primary novel designs.\nFirstly, we introduce the Multi-Head Mixed Convolution (MHMC) module, which can\ncapture multi-scale features and expand the receptive field. Secondly, we\npropose the Scale-Aware Aggregation (SAA) module, which is lightweight but\neffective, enabling information fusion across different heads. By leveraging\nthese two modules, convolutional modulation is further enhanced. Furthermore,\nin contrast to prior works that utilized modulations throughout all stages to\nbuild an attention-free network, we propose an Evolutionary Hybrid Network\n(EHN), which can effectively simulate the shift from capturing local to global\ndependencies as the network becomes deeper, resulting in superior performance.\nExtensive experiments demonstrate that SMT significantly outperforms existing\nstate-of-the-art models across a wide range of visual tasks. Specifically, SMT\nwith 11.5M / 2.4GFLOPs and 32M / 7.7GFLOPs can achieve 82.2% and 84.3% top-1\naccuracy on ImageNet-1K, respectively. After pretrained on ImageNet-22K in\n224^2 resolution, it attains 87.1% and 88.1% top-1 accuracy when finetuned with\nresolution 224^2 and 384^2, respectively. For object detection with Mask R-CNN,\nthe SMT base trained with 1x and 3x schedule outperforms the Swin Transformer\ncounterpart by 4.2 and 1.3 mAP on COCO, respectively. For semantic segmentation\nwith UPerNet, the SMT base test at single- and multi-scale surpasses Swin by\n2.0 and 1.1 mIoU respectively on the ADE20K.",
        "submission": "2023-07-17T15:47:48.000000Z"
    },
    {
        "arxivID": "2307.07947",
        "pdf_url": "https://arxiv.org/pdf/2307.07947.pdf",
        "arxiv_url": "https://arxiv.org/abs/2307.07947",
        "cover": "https://cdn-uploads.huggingface.co/production/uploads/60f1abe7544c2adfd699860c/xCFXhIsJBpjmhrJQj9W4L.png",
        "media": null,
        "field": "Computer Vision and Pattern Recognition",
        "title": "Language Conditioned Traffic Generation",
        "authors": [
            "Shuhan Tan",
            "Boris Ivanovic",
            "Xinshuo Weng",
            "Marco Pavone",
            "Philipp Kraehenbuehl"
        ],
        "desc": "Simulation forms the backbone of modern self-driving development. Simulators\nhelp develop, test, and improve driving systems without putting humans,\nvehicles, or their environment at risk. However, simulators face a major\nchallenge: They rely on realistic, scalable, yet interesting content. While\nrecent advances in rendering and scene reconstruction make great strides in\ncreating static scene assets, modeling their layout, dynamics, and behaviors\nremains challenging. In this work, we turn to language as a source of\nsupervision for dynamic traffic scene generation. Our model, LCTGen, combines a\nlarge language model with a transformer-based decoder architecture that selects\nlikely map locations from a dataset of maps, and produces an initial traffic\ndistribution, as well as the dynamics of each vehicle. LCTGen outperforms prior\nwork in both unconditional and conditional traffic scene generation in terms of\nrealism and fidelity. Code and video will be available at\nthis https URL.",
        "submission": "2023-07-16T05:10:32.000000Z"
    }
]